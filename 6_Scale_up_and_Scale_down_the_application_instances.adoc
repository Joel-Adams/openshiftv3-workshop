[[scale-up-and-scale-down-and-idle-the-application-instances]]
Scale up, Scale down and Idle the application instances
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In this exercise, we will learn how to scale our application. OpenShift
has the capability to scale an application and make sure that many
instances are always running and available.

*Step 1: Switch to an existing project*

For this exercise, we will be using the `mycliproject-UserName` created in a previous lab. Make sure you are switched to that project by using the
`oc project` command and *remember* to substitute in your assigned username for *UserName*.

....
$ oc project mycliproject-UserName
....

*Step 2: View the deployment config*

Take a look at the `deploymentConfig` (or `dc`) of the `time`
application

....
$ oc get deploymentConfig/time -o yaml

apiVersion: v1
kind: DeploymentConfig
metadata:
  annotations:
    openshift.io/generated-by: OpenShiftNewApp
  creationTimestamp: 2016-10-06T05:47:52Z
  generation: 2
  labels:
    app: time
  name: time
  namespace: mycliproject-admin
  resourceVersion: "32084"
  selfLink: /oapi/v1/namespaces/mycliproject-admin/deploymentconfigs/time
  uid: 6bb299e0-8b88-11e6-ba5b-080027782cf7
spec:
  replicas: 1
  selector:
    app: time
    deploymentconfig: time
  strategy:
    resources: {}
    rollingParams:
      intervalSeconds: 1
      maxSurge: 25%
      maxUnavailable: 25%
      timeoutSeconds: 600
      updatePeriodSeconds: 1
    type: Rolling
  template:
    metadata:
      annotations:
        openshift.io/container.time.image.entrypoint: '["sh"]'
        openshift.io/generated-by: OpenShiftNewApp
      creationTimestamp: null
      labels:
        app: time
        deploymentconfig: time
    spec:
      containers:
      - image: 172.30.89.28:5000/mycliproject-admin/time@sha256:c490ea632c5362be3a3985285c623e674e58b876e70d9e3f94a151785b2ee87c
        imagePullPolicy: Always
        name: time
        ports:
        - containerPort: 8080
          protocol: TCP
        resources: {}
        terminationMessagePath: /dev/termination-log
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      securityContext: {}
      terminationGracePeriodSeconds: 30
  test: false
  triggers:
  - type: ConfigChange
  - imageChangeParams:
      automatic: true
      containerNames:
      - time
      from:
        kind: ImageStreamTag
        name: time:latest
        namespace: mycliproject-admin
      lastTriggeredImage: 172.30.89.28:5000/mycliproject-admin/time@sha256:c490ea632c5362be3a3985285c623e674e58b876e70d9e3f94a151785b2ee87c
    type: ImageChange
status:
  availableReplicas: 1
  details:
    causes:
    - imageTrigger:
        from:
          kind: ImageStreamTag
          name: time:latest
          namespace: mycliproject-admin
      type: ImageChange
    message: caused by an image change
  latestVersion: 1
  observedGeneration: 2
  replicas: 1
  updatedReplicas: 1
....

Note that `replicas:` is set to `1`. This tells OpenShift that when
this application deploys, make sure that there is 1 instance.

The `replicationController` mirrors this configuration initially; the
`replicationController` (or `rc`) will ensure that specified number of instances
will always be running.

To view the `rc` for your application, first get the current pod running.

....
oc get pods

NAME           READY     STATUS      RESTARTS   AGE
time-1-45jtc   1/1       Running     0          2h
time-1-build   0/1       Completed   0          2h
....

The above output shows that the build `time-1` is running in pod `45jtc`. Let's
view the `rc` for this build...

....
$ oc get rc/time-1

NAME      DESIRED   CURRENT   AGE
time-1    1         1         2h
....

*Note:* The number of replicas can be changed in the `DeploymentConfig` or
the `ReplicationController`.

However, understand that if you change the `deploymentConfig` it applies to
the associated application. This means, that even if you delete the current replication
controller, the new replication controller that gets created will be assigned the REPLICAS
value based on what is set for the `deploymentConfig`. If you raise the number of desired replicas for the Replication
Controller, the application will scale up. But if, for whatever reason, the current replication controller happens to be deleted, then the newly increased replica setting while be lost.

*Step 3: Scale Application*

To scale the application, edit the `deploymentConfig`.

In the browser, navigate to the `time` Overview page and note that there is only one
instance running.

image:images/scale_updown_overview.png[image]

Now scale the `time` application using the `oc scale` command (remembering to
specify the `dc`):

....
$ oc scale --replicas=3 dc/time
deploymentconfig "time" scaled
....

Back on the web console, we should now see that there are 3
instances running now
image:images/scale_updown_overview_scaled.png[image]

*Note:* You can also scale up and down from the web console by going to
the project overview page and clicking twice on
image:images/scale_up.jpg[image] right next to the pod count circle to
add 2 more pods.

Over on the command line, query the number of pods currently running:

....
$ oc get pods

NAME           READY     STATUS      RESTARTS   AGE
time-1-33wyq   1/1       Running     0          10m
time-1-45jtc   1/1       Running     0          2h
time-1-5ekuk   1/1       Running     0          10m
time-1-build   0/1       Completed   0          2h
....

You now have 3 instances of `time-1` running (each with a different
pod-id). If you check the `rc` of the `time-1` build you will see that
it has been updated by the `dc`.

....
$ oc get rc/time-1

NAME      DESIRED   CURRENT   AGE
time-1    3         3         3h
....

*Step 4: Idling the application*

Execute the following command to list the available endpoints:

....
$ oc get endpoints
NAME      ENDPOINTS                                            AGE
time      10.128.0.33:8080,10.129.0.30:8080,10.129.2.27:8080   15m
....

Note that the name for the endpoints is `time` and there are three IP
addresses defined, one for each of the three pods.

Run the `oc idle endpoints/time` command to idle the application...

....
$ oc idle endpoints/time
Marked service mycliproject-veer/time to unidle resource DeploymentConfig mycliproject-veer/time (unidle to 3 replicas)
Idled DeploymentConfig mycliproject-veer/time (dry run)
....

Go back to the webconsole. Notice that the pods show up as
idled.

image:images/idled_pods.jpeg[image]

At this point the application is idled, the are not any pods running and the application is not utilizing any resources. This does not mean that the application is deleted. The current state is just saved.. thatâ€™s all.

*Step 6: Reactivate your application* Now click on the application route
URL or access the application via curl.

Note that it takes a little while for the application to respond. This
is because pods are spinning up again. You can notice that in the web
console.

In a brief moment, the application will be up and running with 3 pods.

As soon as the application is accessed, it comes up!!!

*Step 7: Scaling Down*

Scaling down involves a similar procedure as scaling up does. Use the `oc scale`
command on the `time` application `dc` setting.

....
oc scale --replicas=1 dc/time

deploymentconfig "time" scaled
....

Alternately, you can go to the project overview page and click on
image:images/scale_down.jpg[image] twice to remove 2 running pods.

Congratulations!! In this exercise we have covered scaling and stepped through the process of
how to scale up/down an application on OpenShift!

link:0_toc.adoc[Table Of Contents]
